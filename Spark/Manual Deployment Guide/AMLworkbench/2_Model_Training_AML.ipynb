{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from azureml.logging import get_azureml_logger\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "# Fill in your Azure storage account information here\n",
    "account_name = 'readmitguideyz'\n",
    "\n",
    "# initialize logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "data_filename = 'wasb://model@{}.blob.core.windows.net/trainingdata'.format(account_name)\n",
    "df = spark.read.parquet(data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed=0)\n",
    "train = train.sampleBy('label', fractions={0.0: 0.2, 1.0: 0.8}, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_model = RandomForestClassifier(featuresCol='features', labelCol='label').fit(train)\n",
    "\n",
    "# Define the classifier   \n",
    "clf = RandomForestClassifier(seed=0)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "paramGrid = ParamGridBuilder().addGrid(clf.maxDepth, [5, 10]).addGrid(clf.maxBins, [32, 64]).build()\n",
    "\n",
    "# Create 3-fold CrossValidator\n",
    "cv = CrossValidator(estimator=clf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "\n",
    "# Run cross validations.  This can take up-to 5 minutes since there 2*2=4 parameter settings for each model, each of which trains with 3 traing set \n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "# Get the best model\n",
    "trained_model = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'wasb://model@{}.blob.core.windows.net/model'.format(account_name)\n",
    "trained_model.save(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6632974750185288"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on test dataset. \n",
    "predictions = trained_model.transform(test)\n",
    "\n",
    "# Evaluate the best trained model on the test dataset with default metric \"areaUnderROC\"\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction    0.0  1.0\n",
      "label                 \n",
      "0.0         17928  220\n",
      "1.0          2270  144\n"
     ]
    }
   ],
   "source": [
    "# Create the confusion matrix for the multiclass prediction results\n",
    "# This result assumes a decision boundary of p = 0.5\n",
    "\n",
    "pred_pd = predictions.toPandas()\n",
    "confuse = pd.crosstab(pred_pd['label'],pred_pd['prediction'])\n",
    "confuse.columns = confuse.columns.map(str)\n",
    "print(confuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.878903\n",
      "Precision = 0.395604\n",
      "Recall = 0.059652\n",
      "F1 = 0.103672\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azureml.logging.script_run_request.ScriptRunRequest at 0x7f647c4ef6d8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select (prediction, true label) and compute test error\n",
    "# True positives - diagonal failure terms \n",
    "tp = confuse['1.0'][1]\n",
    "\n",
    "# False positves - All failure terms - True positives\n",
    "fp = np.sum(np.sum(confuse[['1.0']])) - tp\n",
    "\n",
    "# True negatives \n",
    "tn = confuse['0.0'][0]\n",
    "\n",
    "# False negatives total of non-failure column - TN\n",
    "fn = np.sum(np.sum(confuse[['0.0']])) - tn\n",
    "\n",
    "\n",
    "# Accuracy is diagonal/total \n",
    "acc_n = tn + tp\n",
    "acc_d = np.sum(np.sum(confuse[['0.0','1.0']]))\n",
    "acc = acc_n/acc_d\n",
    "\n",
    "# Calculate precision and recall.\n",
    "prec = tp/(tp+fp)\n",
    "rec = tp/(tp+fn)\n",
    "\n",
    "# Print the evaluation metrics to the notebook\n",
    "print(\"Accuracy = %g\" % acc)\n",
    "print(\"Precision = %g\" % prec)\n",
    "print(\"Recall = %g\" % rec )\n",
    "print(\"F1 = %g\" % (2.0 * prec * rec/(prec + rec)))\n",
    "print(\"\")\n",
    "\n",
    "# logger writes information back into the AML Workbench run time page.\n",
    "# Each title (i.e. \"Model Accuracy\") can be shown as a graph to track\n",
    "# how the metric changes between runs.\n",
    "logger.log(\"Model Accuracy\", (acc))\n",
    "logger.log(\"Model Precision\", (prec))\n",
    "logger.log(\"Model Recall\", (rec))\n",
    "logger.log(\"Model F1\", (2.0 * prec * rec/(prec + rec)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readmitSparkCluster myspark",
   "language": "python",
   "name": "readmitsparkcluster_myspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
