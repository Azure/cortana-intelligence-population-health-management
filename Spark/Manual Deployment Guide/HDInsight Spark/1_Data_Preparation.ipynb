{"nbformat_minor": 2, "cells": [{"source": "# Data Preparation\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "import os, tempfile, zipfile, StringIO\nfrom urllib import urlretrieve\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import DoubleType, StringType\n\ntry:\n    from azure.storage.blob import BlobService\nexcept ImportError:\n    try:\n        from azure.storage.blob import BlockBlobService as BlobService\n    except ImportError:\n        raise Exception('Please ensure that the azure-storage package is installed')\n\n# Fill in your Azure storage account information here\naccount_name = ''\naccount_key = ''", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1493038940826_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-readmi.rcj4qnjbdbfeli1unhyg0yhflf.jx.internal.cloudapp.net:8088/proxy/application_1493038940826_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.15:30060/node/containerlogs/container_e05_1493038940826_0003_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"source": "## Obtain the input dataset\n\nThis tutorial uses a [diabetes dataset](https://archive.ics.uci.edu/ml/datasets/Diabetes) originally produced for the 1994 AAI Spring Symposium on Artificial Intelligence in Medicine, now generously shared by Dr. Michael Kahn on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\n\nTo obtain this dataset and copy it to blob storage, run the code cell below:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "blob_service = BlobService(account_name, account_key)\nblob_service.create_container('preprocess')\n\nwith tempfile.NamedTemporaryFile() as f:\n    urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/00296/dataset_diabetes.zip',\n                f.name)\n    my_file = zipfile.ZipFile(f.name)\n    csv_contents = my_file.read('dataset_diabetes/diabetic_data.csv')\n\ntry:    \n    blob_service.put_block_blob_from_text('preprocess',\n                                          'diabetic_data.csv',\n                                          csv_contents,\n                                          x_ms_blob_content_type='text')\nexcept AttributeError:\n    blob_service.create_blob_from_text('preprocess',\n                                       'diabetic_data.csv',\n                                       csv_contents)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Load data and randomly generate glucose readings\n\nReload the data from blob storage as a Spark dataframe.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "input_filename = 'wasb://preprocess@{}.blob.core.windows.net/diabetic_data.csv'.format(account_name)\ndf = sqlContext.read.csv(input_filename, header=True, sep=',', inferSchema=True)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Generate some glucose readings (which unfortunately are not predictive of anything):", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "def add_noise(x):\n    return x + round(np.random.uniform(0, 0.5), 2)\n\ndf = df.withColumn('discharge_date', F.lit('2015-01-01'))\ndf = df.withColumn('glucose_min', F.lit(0))\ndf = df.withColumn('glucose_max', F.lit(15))\ndf = df.withColumn('glucose_mean', F.lit(5))\ndf = df.withColumn('glucose_var', F.lit(9))\n\nudf_add_noise = F.udf(add_noise, DoubleType())\ndf = df.withColumn('glucose_min', udf_add_noise(df['glucose_min']))\ndf = df.withColumn('glucose_max', udf_add_noise(df['glucose_max']))\ndf = df.withColumn('glucose_mean', udf_add_noise(df['glucose_mean']))\ndf = df.withColumn('glucose_var', udf_add_noise(df['glucose_var']))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Handle missing values", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "df = df.select([F.when(df[c].cast('string') != \"?\", F.col(c)).otherwise(None).alias(c) for c in df.columns])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Add indicator columns for numeric and categorical missing values. (Retain the id columns for merging with other dataframes.)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "# Define which variables are in which categories\nid_vars = ['encounter_id', 'patient_nbr', 'discharge_date']\nlabel_var = ['readmitted']\nnum_vars = ['time_in_hospital', 'num_lab_procedures', 'num_procedures',\n            'num_medications', 'number_outpatient', 'number_emergency',\n            'number_inpatient', 'diag_1', 'diag_2', 'diag_3', 'number_diagnoses',\n            'glucose_min', 'glucose_max', 'glucose_mean', 'glucose_var']\ncat_vars = ['race', 'gender', 'age', 'weight', 'admission_type_id',\n            'discharge_disposition_id', 'admission_source_id',\n            'payer_code', 'medical_specialty',\n            'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide',\n            'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',\n            'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n            'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',\n            'insulin', 'glyburide-metformin', 'glipizide-metformin',\n            'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n            'metformin-pioglitazone', 'change', 'diabetesMed']\n\ndf_mvi = df.select(id_vars + [F.when(df[c].isNull(), 'y').otherwise('n').alias(c + '_missing') for c in num_vars + cat_vars])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Replace missing numeric values with the column means:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "df_num = df.select(id_vars + [df[c].cast('double') for c in num_vars])\nnum_var_means = dict(zip(num_vars,\n                         df_num.select([F.mean(df_num[c]).alias(c + '_mean') \\\n                                        for c in num_vars]).rdd.flatMap(lambda x: x).collect()))\ndf_num = df_num.select(id_vars + [F.when(df_num[c].isNull(), num_var_means[c]).otherwise(df_num[c]).alias(c) for c in num_vars])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Indicate missing values in categorical columns. Merge with other missing value indicators.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "df_cat = df.select(id_vars + [F.when(df[c].isNull(), 'NA_').otherwise(df[c].cast('string')).alias(c) for c in cat_vars])\ndf_cat = df_cat.join(df_mvi, id_vars, 'inner')\ncat_vars = [x for x in df_cat.columns if x not in id_vars]", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Create the string indexing pipeline (takes a while)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "s_indexers = [StringIndexer(inputCol=x, outputCol=x + '__indexed__') for x in cat_vars]\nsi_pipe = Pipeline(stages=s_indexers)\nsi_pipe_model = si_pipe.fit(df_cat)\ndf_cat = si_pipe_model.transform(df_cat)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Save the pipeline:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "si_pipe_model_filename = 'wasb://model@{}.blob.core.windows.net/si_pipe_model'.format(account_name)\nsi_pipe_model.write().overwrite().save(si_pipe_model_filename)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Remove from consideration any categorical variables that have only one level:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "cat_col_var = df_cat.select([F.variance(df_cat[c]).alias(c + '_sd') for \\\n                             c in [cv + '__indexed__' for cv in cat_vars]]).rdd.flatMap(lambda x: x).collect()\ncat_vars = [cat_vars[i] for i in range(len(cat_col_var)) if cat_col_var[i] != 0]", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Perform one-hot encoding", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "oh_encoders = [OneHotEncoder(inputCol=x + '__indexed__', outputCol=x + '__encoded__')\n              for x in cat_vars]\ndf_cat = df_cat.select(id_vars + [x + '__indexed__' for x in cat_vars])\noh_pipe_model = Pipeline(stages=oh_encoders).fit(df_cat)\ndf_cat = oh_pipe_model.transform(df_cat)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Save the pipeline:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "oh_pipe_model_filename = 'wasb://model@{}.blob.core.windows.net/oh_pipe_model'.format(account_name)\noh_pipe_model.write().overwrite().save(oh_pipe_model_filename)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Assemble categorical features into one vector", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "df_cat = df_cat.select([df_cat[c].alias(c.replace('__encoded__', ''))\n                         for c in id_vars + [x + '__encoded__' for x in cat_vars]])\nva = VectorAssembler(inputCols=cat_vars, outputCol='cat_features')\ndf_cat = va.transform(df_cat).select(id_vars + ['cat_features'])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Map ambiguous labels appropriately", "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "label_map = {'NO': 0, '>30': 0, '<30': 1}\ndef map_label(label):\n    return(label_map[label])\n\ndf_label = df.select(id_vars + label_var)\nudf_map_label = F.udf(map_label, StringType())\ndf_label = df_label.withColumn('readmitted', udf_map_label(df_label['readmitted']))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Create string indexer:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "m_si_label = StringIndexer(inputCol='readmitted', outputCol='label').fit(df_label)\ndf_label = m_si_label.transform(df_label)\ndf_label = df_label.drop('readmitted')", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Save string indexer:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": "si_label_filename = 'wasb://model@{}.blob.core.windows.net/si_label'.format(account_name)\nm_si_label.write().overwrite().save(si_label_filename)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Merge dataframes back together", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "df = df_label.join(df_num, id_vars, 'inner').join(df_cat, id_vars, 'inner')", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": "va = VectorAssembler(inputCols=(num_vars + ['cat_features']), outputCol='features')\ndf = va.transform(df).select('label', 'features')", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Save the preprocessed data for model training", "cell_type": "markdown", "metadata": {}}, {"execution_count": 30, "cell_type": "code", "source": "data_filename = 'wasb://model@{}.blob.core.windows.net/trainingdata'.format(account_name)\ndf.write.parquet(data_filename)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "# Generate imaginary patients for the simulator", "cell_type": "markdown", "metadata": {}}, {"source": "We will train and evaluate the model using the historical patient data preprocessed above. However, we will generate new patients and streaming glucose level readings to demonstrate how our model can be applied to incoming patient data. Here, we generate the imaginary patient profiles:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 31, "cell_type": "code", "source": "# number of patients to simulate\nnum_patients = 100\n\n# get the distributions of numerical and categorical features in the real data\ndf = sqlContext.read.csv(input_filename, header=True, sep=',', inferSchema=True)\n\nnum_vars = ['time_in_hospital', 'num_lab_procedures', 'num_procedures',\n            'num_medications', 'number_outpatient', 'number_emergency',\n            'number_inpatient', 'diag_1', 'diag_2', 'diag_3', 'number_diagnoses']\ncat_vars = ['race', 'gender', 'age', 'weight', 'admission_type_id',\n            'discharge_disposition_id', 'admission_source_id',\n            'payer_code', 'medical_specialty',\n            'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide',\n            'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',\n            'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n            'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',\n            'insulin', 'glyburide-metformin', 'glipizide-metformin',\n            'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n            'metformin-pioglitazone', 'change', 'diabetesMed']\n\ndistrib_dict = {}\nfor column in cat_vars:\n    column_dist = iter(df.groupBy(column).count().rdd.flatMap(lambda x: x).collect())\n    column_dict = dict(zip(column_dist, column_dist))\n    distrib_dict[column] = column_dict\n    \n    \nfor column in num_vars:\n    column_mean = df.agg(F.mean(F.col(column))).rdd.flatMap(lambda x: x).collect()\n    column_stddev = df.agg(F.stddev(F.col(column))).rdd.flatMap(lambda x: x).collect()\n    entry = (column_mean[0], column_stddev[0])\n    distrib_dict[column] = entry\n\n# remove values that indicate missingness\nkeys_to_remove = ['?', 'Unknown/Invalid', 'Other', 'PhysicianNotFound', 'None']\nfor key in distrib_dict.keys():\n    if type(distrib_dict[key]) != dict:\n        continue\n    for key_to_remove in keys_to_remove:\n        if key_to_remove in distrib_dict[key]:\n            del distrib_dict[key][key_to_remove]\n            \nrandom_values = pd.DataFrame(np.random.randint(500000000, size=num_patients).T.tolist(),\n                             columns=['patient_nbr'])\nfor column in cat_vars:\n    my_dict = distrib_dict[column]\n    possible_values = list(my_dict.keys())\n    likelihoods = [my_dict[key] for key in possible_values]\n    likelihoods = [float(i) / sum(likelihoods) for i in likelihoods]\n    random_values[column] = np.random.choice(possible_values, num_patients, p=likelihoods)\n    \nfor column in num_vars:\n    my_mean, my_stddev = distrib_dict[column]\n    random_values[column] = np.random.normal(my_mean, my_stddev, num_patients)\n    \ndf_columns = df.columns\ndf_columns.pop(df_columns.index('readmitted'))  # we include no label for these patients\ndf_columns.pop(df_columns.index('encounter_id'))  # info in the patient record will be consistent across encounters\nrandom_values = random_values[df_columns]", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Upload the newly-generated patient records:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 32, "cell_type": "code", "source": "blob_service.create_container('patientrecords')\n\ns = StringIO.StringIO()\nrandom_values.to_csv(s, index=False)\nstrings = s.getvalue().split('\\n')[:-1]\nheader = strings.pop(0)\n\nfor string in strings:\n    csv_contents = '\\n'.join([header, string])\n    nbr = string.split(',')[0]\n    try:\n        blob_service.put_block_blob_from_text('patientrecords',\n                                              '{}.csv'.format(nbr),\n                                              csv_contents,\n                                              x_ms_blob_content_type='text')\n    except AttributeError:\n        blob_service.create_blob_from_text('patientrecords',\n                                           '{}.csv'.format(nbr),\n                                           csv_contents)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "About 5-10 minutes after your patient records have been copied to blob storage, you should see simulated glucose levels begin to appear in your storage account's `glucoselevelsaggs` container.", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}
