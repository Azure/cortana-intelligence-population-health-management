{"nbformat_minor": 2, "cells": [{"source": "# Model Training", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "import os\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\n# Fill in your Azure storage account information here\naccount_name = ''", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Load the preprocessed data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "data_filename = 'wasb://model@{}.blob.core.windows.net/trainingdata'.format(account_name)\ndf = sqlContext.read.parquet(data_filename)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Partition the data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "train, test = df.randomSplit([0.8, 0.2], seed=0)\ntrain = train.sampleBy('label', fractions={0.0: 0.2, 1.0: 0.8}, seed=0)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Train the model", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "trained_model = RandomForestClassifier(featuresCol='features', labelCol='label').fit(train)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Store the model:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "model_filename = 'wasb://model@{}.blob.core.windows.net/model'.format(account_name)\ntrained_model.save(model_filename)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Evaluate the model", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "mc_evaluator = MulticlassClassificationEvaluator(labelCol='label')\n\npredictions = trained_model.transform(test)\nprecision = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: \"weightedPrecision\"})\nrecall = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: \"weightedRecall\"})\naccuracy = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: \"accuracy\"})\nprint('Accuracy: {:0.3f}\\nWeighted precision: {:0.3f}\\nWeighted recall: {:0.3f}\\n'.format(accuracy, precision, recall))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Accuracy: 0.883\nWeighted precision: 0.836\nWeighted recall: 0.883"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}