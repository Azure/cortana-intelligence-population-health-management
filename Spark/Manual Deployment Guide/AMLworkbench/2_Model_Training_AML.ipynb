{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path, makedirs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from azureml.logging import get_azureml_logger\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "# Fill in your Azure storage account information here\n",
    "account_name = ''\n",
    "\n",
    "# initialize logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "data_filename = 'wasb://model@{}.blob.core.windows.net/trainingdata'.format(account_name)\n",
    "df = spark.read.parquet(data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed=0)\n",
    "train = train.sampleBy('label', fractions={0.0: 0.2, 1.0: 0.8}, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Many machine learning algorithms have one or more knobs, called hyperparameters. These knobs allow tuning of algorithms to optimize their performance over future data, measured according to user-specified metrics (for example, accuracy, AUC, RMSE). Data scientist needs to provide values of hyperparameters when building a model over training data and before seeing the future test data. How based on the known training data can we set up the values of hyperparameters so that the model has a good performance over the unknown test data? \n",
    "\n",
    "A popular technique for tuning hyperparameters is a grid search combined with cross-validation. Cross-validation is a technique that assesses how well a model, trained on a training set, predicts over the test set. Using this technique, initially we divide the dataset into K folds and then train the algorithm K times, in a round-robin fashion, on all but one of the folds, called held-out fold. We compute the average value of the metrics of K models over K held-out folds. This average value, called cross-validated performance estimate, depends on the values of hyperparameters used when creating K models. When tuning hyperparameters, we search through the space of candidate hyperparameter values to find the ones that optimize cross-validation performance estimate. Grid search is a common search technique, where the space of candidate values of multiple hyperparameters is a cross-product of sets of candidate values of individual hyperparameters. \n",
    "\n",
    "Grid search using cross-validation can be time-consuming. If an algorithm has 5 hyperparameters, each with 5 candidate values and we use K=5 folds, then to complete a grid search we need to train 56=15625 models. Fortunately, grid-search using cross-validation is an embarrassingly parallel procedure and all these models can be trained in parallel.\n",
    "\n",
    "In this particular case, the grid has four combinations of values of hyperparameters (maxDepth and maxBins). We use 3-fold cross validation, resulting 4x3=12 runs of RandomForestClassifier. Note that the default metric for the BinaryClassificationEvaluator is `areaUnderROC`. To measure performance of the models, we use negative log loss metric. The following code finds the best model which applies the values of hyperparameters from the grid that maximize the cross-validated `areaUnderROC`. We use this best model as our trained\\_model, which is to be saved for future scoring.\n",
    "\n",
    "For more information about hyperparameter tuning with Azure ML Workbench please refer to article [Distributed tuning of hyperparameters using Azure Machine Learning Workbench](https://docs.microsoft.com/en-us/azure/machine-learning/preview/scenario-distributed-tuning-of-hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_model = RandomForestClassifier(featuresCol='features', labelCol='label').fit(train)\n",
    "\n",
    "# Define the classifier   \n",
    "clf = RandomForestClassifier(seed=0)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "paramGrid = ParamGridBuilder().addGrid(clf.maxDepth, [5, 10]).addGrid(clf.maxBins, [32, 64]).build()\n",
    "\n",
    "# Create 3-fold CrossValidator\n",
    "cv = CrossValidator(estimator=clf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "\n",
    "# Run cross validations.  This can take up-to 5 minutes since there 2*2=4 parameter settings for each model, each of which trains with 3 traing set \n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "# Get the best model\n",
    "trained_model = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluation metric is areaUnderROC.\n",
      "Parameter MaxDepth of the best model is 5.\n",
      "Parameter MaxBins of the best model is 32.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azureml.logging.script_run_request.ScriptRunRequest at 0x7f6ea6da7358>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The evaluation metric is {}.\".format(evaluator.getMetricName()))\n",
    "\n",
    "print(\"Parameter MaxDepth of the best model is {}.\".format(clf.getMaxDepth()))\n",
    "print(\"Parameter MaxBins of the best model is {}.\".format(clf.getMaxBins()))\n",
    "\n",
    "logger.log(\"MaxDepth\", (clf.getMaxDepth()))\n",
    "logger.log(\"MaxBins\", (clf.getMaxBins()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the model\n",
    "model_filename = 'wasb://model@{}.blob.core.windows.net/model'.format(account_name)\n",
    "\n",
    "trained_model.save(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6632974750185294"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on test dataset. \n",
    "predictions = trained_model.transform(test)\n",
    "\n",
    "# Evaluate the best trained model on the test dataset with default metric \"areaUnderROC\"\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction    0.0  1.0\n",
      "label                 \n",
      "0.0         17928  220\n",
      "1.0          2270  144\n"
     ]
    }
   ],
   "source": [
    "# Create the confusion matrix for the multiclass prediction results\n",
    "# This result assumes a decision boundary of p = 0.5\n",
    "\n",
    "pred_pd = predictions.toPandas()\n",
    "confuse = pd.crosstab(pred_pd['label'],pred_pd['prediction'])\n",
    "confuse.columns = confuse.columns.map(str)\n",
    "print(confuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.878903\n",
      "Precision = 0.395604\n",
      "Recall = 0.059652\n",
      "F1 = 0.103672\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azureml.logging.script_run_request.ScriptRunRequest at 0x7f6e9c1a4208>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select (prediction, true label) and compute test error\n",
    "# True positives - diagonal failure terms \n",
    "tp = confuse['1.0'][1]\n",
    "\n",
    "# False positves - All failure terms - True positives\n",
    "fp = np.sum(np.sum(confuse[['1.0']])) - tp\n",
    "\n",
    "# True negatives \n",
    "tn = confuse['0.0'][0]\n",
    "\n",
    "# False negatives total of non-failure column - TN\n",
    "fn = np.sum(np.sum(confuse[['0.0']])) - tn\n",
    "\n",
    "\n",
    "# Accuracy is diagonal/total \n",
    "acc_n = tn + tp\n",
    "acc_d = np.sum(np.sum(confuse[['0.0','1.0']]))\n",
    "acc = acc_n/acc_d\n",
    "\n",
    "# Calculate precision and recall.\n",
    "prec = tp/(tp+fp)\n",
    "rec = tp/(tp+fn)\n",
    "\n",
    "# Print the evaluation metrics to the notebook\n",
    "print(\"Accuracy = %g\" % acc)\n",
    "print(\"Precision = %g\" % prec)\n",
    "print(\"Recall = %g\" % rec )\n",
    "print(\"F1 = %g\" % (2.0 * prec * rec/(prec + rec)))\n",
    "print(\"\")\n",
    "\n",
    "# logger writes information back into the AML Workbench run time page.\n",
    "# Each title (i.e. \"Model Accuracy\") can be shown as a graph to track\n",
    "# how the metric changes between runs.\n",
    "logger.log(\"Model Accuracy\", (acc))\n",
    "logger.log(\"Model Precision\", (prec))\n",
    "logger.log(\"Model Recall\", (rec))\n",
    "logger.log(\"Model F1\", (2.0 * prec * rec/(prec + rec)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readmitSparkCluster myspark",
   "language": "python",
   "name": "readmitsparkcluster_myspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
