{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'readmitguideyz'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, tempfile, zipfile\n",
    "from io import StringIO\n",
    "#from urllib import urlretrieve\n",
    "from urllib.request import urlretrieve \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "try:\n",
    "    from azure.storage.blob import BlobService\n",
    "except ImportError:\n",
    "    try:\n",
    "        from azure.storage.blob import BlockBlobService as BlobService\n",
    "    except ImportError:\n",
    "        raise Exception('Please ensure that the azure-storage package is installed')\n",
    "\n",
    "# Fill in your Azure storage account information here\n",
    "account_name = ''\n",
    "account_key = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the input dataset\n",
    "\n",
    "This tutorial uses a [diabetes dataset](https://archive.ics.uci.edu/ml/datasets/Diabetes) originally produced for the 1994 AAI Spring Symposium on Artificial Intelligence in Medicine, now generously shared by Dr. Michael Kahn on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\n",
    "\n",
    "To obtain this dataset and copy it to blob storage, run the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service = BlobService(account_name, account_key)\n",
    "blob_service.create_container('preprocess')\n",
    "\n",
    "with tempfile.NamedTemporaryFile() as f:\n",
    "    urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/00296/dataset_diabetes.zip',\n",
    "                f.name)\n",
    "    my_file = zipfile.ZipFile(f.name)\n",
    "    csv_contents = my_file.read('dataset_diabetes/diabetic_data.csv')\n",
    "\n",
    "try:    \n",
    "    blob_service.put_block_blob_from_text('preprocess',\n",
    "                                          'diabetic_data.csv',\n",
    "                                          csv_contents,\n",
    "                                          x_ms_blob_content_type='text')\n",
    "except AttributeError:\n",
    "    blob_service.create_blob_from_text('preprocess',\n",
    "                                       'diabetic_data.csv',\n",
    "                                       csv_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and randomly generate glucose readings\n",
    "\n",
    "Reload the data from blob storage as a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "input_filename = 'wasb://preprocess@{}.blob.core.windows.net/diabetic_data.csv'.format(account_name)\n",
    "df = spark.read.csv(input_filename, header=True, sep=',', inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some glucose readings (which unfortunately are not predictive of anything):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(x):\n",
    "    return x + round(np.random.uniform(0, 0.5), 2)\n",
    "\n",
    "df = df.withColumn('discharge_date', F.lit('2015-01-01'))\n",
    "df = df.withColumn('glucose_min', F.lit(0))\n",
    "df = df.withColumn('glucose_max', F.lit(15))\n",
    "df = df.withColumn('glucose_mean', F.lit(5))\n",
    "df = df.withColumn('glucose_var', F.lit(9))\n",
    "\n",
    "udf_add_noise = F.udf(add_noise, DoubleType())\n",
    "df = df.withColumn('glucose_min', udf_add_noise(df['glucose_min']))\n",
    "df = df.withColumn('glucose_max', udf_add_noise(df['glucose_max']))\n",
    "df = df.withColumn('glucose_mean', udf_add_noise(df['glucose_mean']))\n",
    "df = df.withColumn('glucose_var', udf_add_noise(df['glucose_var']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([F.when(df[c].cast('string') != \"?\", F.col(c)).otherwise(None).alias(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add indicator columns for numeric and categorical missing values. (Retain the id columns for merging with other dataframes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which variables are in which categories\n",
    "id_vars = ['encounter_id', 'patient_nbr', 'discharge_date']\n",
    "label_var = ['readmitted']\n",
    "num_vars = ['time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
    "            'num_medications', 'number_outpatient', 'number_emergency',\n",
    "            'number_inpatient', 'diag_1', 'diag_2', 'diag_3', 'number_diagnoses',\n",
    "            'glucose_min', 'glucose_max', 'glucose_mean', 'glucose_var']\n",
    "cat_vars = ['race', 'gender', 'age', 'weight', 'admission_type_id',\n",
    "            'discharge_disposition_id', 'admission_source_id',\n",
    "            'payer_code', 'medical_specialty',\n",
    "            'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide',\n",
    "            'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',\n",
    "            'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n",
    "            'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',\n",
    "            'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "            'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "            'metformin-pioglitazone', 'change', 'diabetesMed']\n",
    "\n",
    "df_mvi = df.select(id_vars + [F.when(df[c].isNull(), 'y').otherwise('n').alias(c + '_missing') for c in num_vars + cat_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing numeric values with the column means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df.select(id_vars + [df[c].cast('double') for c in num_vars])\n",
    "num_var_means = dict(zip(num_vars,\n",
    "                         df_num.select([F.mean(df_num[c]).alias(c + '_mean') \\\n",
    "                                        for c in num_vars]).rdd.flatMap(lambda x: x).collect()))\n",
    "df_num = df_num.select(id_vars + [F.when(df_num[c].isNull(), num_var_means[c]).otherwise(df_num[c]).alias(c) for c in num_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicate missing values in categorical columns. Merge with other missing value indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.select(id_vars + [F.when(df[c].isNull(), 'NA_').otherwise(df[c].cast('string')).alias(c) for c in cat_vars])\n",
    "df_cat = df_cat.join(df_mvi, id_vars, 'inner')\n",
    "cat_vars = [x for x in df_cat.columns if x not in id_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the string indexing pipeline (takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_indexers = [StringIndexer(inputCol=x, outputCol=x + '__indexed__') for x in cat_vars]\n",
    "si_pipe = Pipeline(stages=s_indexers)\n",
    "si_pipe_model = si_pipe.fit(df_cat)\n",
    "df_cat = si_pipe_model.transform(df_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_pipe_model_filename = 'wasb://model@{}.blob.core.windows.net/si_pipe_model'.format(account_name)\n",
    "si_pipe_model.write().overwrite().save(si_pipe_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove from consideration any categorical variables that have only one level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col_var = df_cat.select([F.variance(df_cat[c]).alias(c + '_sd') for \\\n",
    "                             c in [cv + '__indexed__' for cv in cat_vars]]).rdd.flatMap(lambda x: x).collect()\n",
    "cat_vars = [cat_vars[i] for i in range(len(cat_col_var)) if cat_col_var[i] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_encoders = [OneHotEncoder(inputCol=x + '__indexed__', outputCol=x + '__encoded__')\n",
    "              for x in cat_vars]\n",
    "df_cat = df_cat.select(id_vars + [x + '__indexed__' for x in cat_vars])\n",
    "oh_pipe_model = Pipeline(stages=oh_encoders).fit(df_cat)\n",
    "df_cat = oh_pipe_model.transform(df_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_pipe_model_filename = 'wasb://model@{}.blob.core.windows.net/oh_pipe_model'.format(account_name)\n",
    "oh_pipe_model.write().overwrite().save(oh_pipe_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble categorical features into one vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df_cat.select([df_cat[c].alias(c.replace('__encoded__', ''))\n",
    "                         for c in id_vars + [x + '__encoded__' for x in cat_vars]])\n",
    "va = VectorAssembler(inputCols=cat_vars, outputCol='cat_features')\n",
    "df_cat = va.transform(df_cat).select(id_vars + ['cat_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map ambiguous labels appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'NO': 0, '>30': 0, '<30': 1}\n",
    "def map_label(label):\n",
    "    return(label_map[label])\n",
    "\n",
    "df_label = df.select(id_vars + label_var)\n",
    "udf_map_label = F.udf(map_label, StringType())\n",
    "df_label = df_label.withColumn('readmitted', udf_map_label(df_label['readmitted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create string indexer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_si_label = StringIndexer(inputCol='readmitted', outputCol='label').fit(df_label)\n",
    "df_label = m_si_label.transform(df_label)\n",
    "df_label = df_label.drop('readmitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save string indexer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_label_filename = 'wasb://model@{}.blob.core.windows.net/si_label'.format(account_name)\n",
    "m_si_label.write().overwrite().save(si_label_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge dataframes back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_label.join(df_num, id_vars, 'inner').join(df_cat, id_vars, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=(num_vars + ['cat_features']), outputCol='features')\n",
    "df = va.transform(df).select('label', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the preprocessed data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = 'wasb://model@{}.blob.core.windows.net/trainingdata'.format(account_name)\n",
    "df.write.parquet(data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate imaginary patients for the simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train and evaluate the model using the historical patient data preprocessed above. However, we will generate new patients and streaming glucose level readings to demonstrate how our model can be applied to incoming patient data. Here, we generate the imaginary patient profiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of patients to simulate\n",
    "num_patients = 100\n",
    "\n",
    "# get the distributions of numerical and categorical features in the real data\n",
    "# df = sqlContext.read.csv(input_filename, header=True, sep=',', inferSchema=True)\n",
    "df = spark.read.csv(input_filename, header=True, sep=',', inferSchema=True)\n",
    "\n",
    "num_vars = ['time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
    "            'num_medications', 'number_outpatient', 'number_emergency',\n",
    "            'number_inpatient', 'diag_1', 'diag_2', 'diag_3', 'number_diagnoses']\n",
    "cat_vars = ['race', 'gender', 'age', 'weight', 'admission_type_id',\n",
    "            'discharge_disposition_id', 'admission_source_id',\n",
    "            'payer_code', 'medical_specialty',\n",
    "            'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide',\n",
    "            'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',\n",
    "            'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n",
    "            'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',\n",
    "            'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "            'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "            'metformin-pioglitazone', 'change', 'diabetesMed']\n",
    "\n",
    "distrib_dict = {}\n",
    "for column in cat_vars:\n",
    "    column_dist = iter(df.groupBy(column).count().rdd.flatMap(lambda x: x).collect())\n",
    "    column_dict = dict(zip(column_dist, column_dist))\n",
    "    distrib_dict[column] = column_dict\n",
    "    \n",
    "    \n",
    "for column in num_vars:\n",
    "    column_mean = df.agg(F.mean(F.col(column))).rdd.flatMap(lambda x: x).collect()\n",
    "    column_stddev = df.agg(F.stddev(F.col(column))).rdd.flatMap(lambda x: x).collect()\n",
    "    entry = (column_mean[0], column_stddev[0])\n",
    "    distrib_dict[column] = entry\n",
    "\n",
    "# remove values that indicate missingness\n",
    "keys_to_remove = ['?', 'Unknown/Invalid', 'Other', 'PhysicianNotFound', 'None']\n",
    "for key in distrib_dict.keys():\n",
    "    if type(distrib_dict[key]) != dict:\n",
    "        continue\n",
    "    for key_to_remove in keys_to_remove:\n",
    "        if key_to_remove in distrib_dict[key]:\n",
    "            del distrib_dict[key][key_to_remove]\n",
    "            \n",
    "random_values = pd.DataFrame(np.random.randint(500000000, size=num_patients).T.tolist(),\n",
    "                             columns=['patient_nbr'])\n",
    "for column in cat_vars:\n",
    "    my_dict = distrib_dict[column]\n",
    "    possible_values = list(my_dict.keys())\n",
    "    likelihoods = [my_dict[key] for key in possible_values]\n",
    "    likelihoods = [float(i) / sum(likelihoods) for i in likelihoods]\n",
    "    random_values[column] = np.random.choice(possible_values, num_patients, p=likelihoods)\n",
    "    \n",
    "for column in num_vars:\n",
    "    my_mean, my_stddev = distrib_dict[column]\n",
    "    random_values[column] = np.random.normal(my_mean, my_stddev, num_patients)\n",
    "    \n",
    "df_columns = df.columns\n",
    "df_columns.pop(df_columns.index('readmitted'))  # we include no label for these patients\n",
    "df_columns.pop(df_columns.index('encounter_id'))  # info in the patient record will be consistent across encounters\n",
    "random_values = random_values[df_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the newly-generated patient records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_service.create_container('patientrecords')\n",
    "\n",
    "#s = StringIO.StringIO()\n",
    "s = StringIO()\n",
    "random_values.to_csv(s, index=False)\n",
    "strings = s.getvalue().split('\\n')[:-1]\n",
    "header = strings.pop(0)\n",
    "\n",
    "for string in strings:\n",
    "    csv_contents = '\\n'.join([header, string])\n",
    "    nbr = string.split(',')[0]\n",
    "    try:\n",
    "        blob_service.put_block_blob_from_text('patientrecords',\n",
    "                                              '{}.csv'.format(nbr),\n",
    "                                              csv_contents,\n",
    "                                              x_ms_blob_content_type='text')\n",
    "    except AttributeError:\n",
    "        blob_service.create_blob_from_text('patientrecords',\n",
    "                                           '{}.csv'.format(nbr),\n",
    "                                           csv_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 5-10 minutes after your patient records have been copied to blob storage, you should see simulated glucose levels begin to appear in your storage account's `glucoselevelsaggs` container."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readmitSparkCluster myspark",
   "language": "python",
   "name": "readmitsparkcluster_myspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
